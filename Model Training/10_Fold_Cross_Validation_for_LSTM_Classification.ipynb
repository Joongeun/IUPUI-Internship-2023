{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10719,
          "status": "ok",
          "timestamp": 1692846791797,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          },
          "user_tz": 240
        },
        "id": "x4giRzM7NtHJ",
        "outputId": "32ada066-03a1-49f0-ed0e-a3a9b9c20e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('always')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1692846791798,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          },
          "user_tz": 240
        },
        "id": "DMV27R008-Jx",
        "outputId": "20a639ba-ee01-48ee-bc71-61436bf4e4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 299,
          "status": "ok",
          "timestamp": 1692846792093,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          },
          "user_tz": 240
        },
        "id": "cwJrQFQgN_BE",
        "outputId": "44598dfb-d7c2-41f2-ba37-6a5415dfc46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    0.522571\n",
              "4.0    0.345714\n",
              "2.0    0.052286\n",
              "3.0    0.043429\n",
              "0.0    0.028286\n",
              "5.0    0.007714\n",
              "Name: labels, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/Labeled Posts - preprocessed_csv.csv\")\n",
        "# check class distribution\n",
        "df['labels'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4K6OKQSZji"
      },
      "source": [
        "# Preprocess/Tokenization/Padding\n",
        "(Convert text into numbers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R6hRrn3JDf0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846792093,
          "user_tz": 240,
          "elapsed": 10,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "a2673297-84d5-4c86-b092-daf6f942a4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def preprocess_string(s):\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "    split = re.findall(r\"[\\w']+|[!?]\", s)\n",
        "    i = 0\n",
        "    while i < len(split)-1:\n",
        "      if split[i] == split[i+1]:\n",
        "        del split[i+1]\n",
        "        i-=1\n",
        "      i+= 1\n",
        "    # Remove all instances of \\n where only the n remains\n",
        "    split = [i for i in split if i != \"n\"]\n",
        "    return \" \".join(split)\n",
        "\n",
        "def tokenize(x_train, y_train):\n",
        "    word_list = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "    corpus = Counter(word_list)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]                #Only using top 1000 words to train model\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "    # tokenize\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)]  for word in sent.lower().split()\n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    encoded_train = [1 if label == 2.0 else 0 for label in y_train]\n",
        "    return np.array(final_list_train), np.array(encoded_train),onehot_dict\n",
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bldHeeBQg2p"
      },
      "source": [
        "# Shuffle Train/Val_data\n",
        "\n",
        "> Get 10 folds (split train_data into validation & train set - 1:9 ratio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1fWmPaxLEOL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846792094,
          "user_tz": 240,
          "elapsed": 7,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "ae00df3f-dc9f-44e1-8cbe-e9956eda8e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 35    #  Number of posts per batch (when training); GCF of 2835, 315, 350 (train, val, test) = 35\n",
        "def shuffler(x, y):\n",
        "  data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))\n",
        "  # make sure to SHUFFLE your data\n",
        "  #train_loader is a list of len 63 - we divided 2835 posts in x_train into 81 batchs of 35 posts\n",
        "  #each of the 63 lists are made of 2 sublists, both of len 35 - first is the padded texts, second is the list of labels\n",
        "  loader = DataLoader(data, shuffle=True, batch_size=batch_size)           # shuffled train_data/test_data\n",
        "  return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mfhSPF5jOWb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794882,
          "user_tz": 240,
          "elapsed": 2792,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "68122cb7-01e8-4e8e-ccae-657ef36a04b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-6-14aa95062a83>:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(final_list_train), np.array(encoded_train),onehot_dict\n"
          ]
        }
      ],
      "source": [
        "X,y = df['selftext'][0:3500].values,df['labels'][0:3500].values\n",
        "#Tokenize\n",
        "X, y, vocab = tokenize(X, y)\n",
        "#Padding\n",
        "X = padding_(X, 500)\n",
        "\n",
        "#Split xtr/ytr into train/val data\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "train_loader, valid_loader, test_loader = [], [], []\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "  xtrain, ytrain, xtest, ytest = [], [], [], []\n",
        "  for j in train_index:\n",
        "    xtrain.append(X[j])\n",
        "    ytrain.append(y[j])\n",
        "  for j in test_index:\n",
        "    xtest.append(X[j])\n",
        "    ytest.append(y[j])\n",
        "\n",
        "  xtrain, ytrain, xtest, ytest = map(np.array, [xtrain, ytrain, xtest, ytest])\n",
        "\n",
        "  #Split into train + val\n",
        "  xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, stratify=ytrain, test_size=0.1)\n",
        "\n",
        "  #Shuffle/make dataloader (batches)\n",
        "  trainloader = shuffler(xtrain, ytrain)\n",
        "  valloader = shuffler(xval, yval)\n",
        "  testloader = shuffler(xtest, ytest)\n",
        "\n",
        "  train_loader.append(trainloader)\n",
        "  valid_loader.append(valloader)\n",
        "  test_loader.append(testloader)\n",
        "\n",
        "#train_loader: 10 folds with 63 batches of 45 posts, valid_loader: 10 folds with 7 batches of 45 posts, test_loader: 10 folds with 8 batches of 45 posts\n",
        "\n",
        "#Train: 2835 posts, Validation: 315 posts, Test: 350 posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQBZ_0sMYmu0"
      },
      "source": [
        "# Create Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "22RjvTe-ExYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794882,
          "user_tz": 240,
          "elapsed": 11,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "7e5cab92-e825-4f1c-aa17-3d263ebd033d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim, num_layers=no_layers, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(embeds.shape)  #[50, 500, 1000]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rqSplN5Yszz"
      },
      "source": [
        "# Define learning rate and accuracy func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "51CKkiKLFfNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794883,
          "user_tz": 240,
          "elapsed": 9,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "628e781c-67db-471a-a799-0813ef552d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "lr = 0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def merge_arrs(pred, label):\n",
        "  new_pred = []\n",
        "  for i in pred:\n",
        "    new_pred.extend(list(i))\n",
        "  pred = np.array(new_pred)\n",
        "  new_label = []\n",
        "  for i in label:\n",
        "    new_label.extend(list(i))\n",
        "  label = np.array(new_label)\n",
        "  return pred, label\n",
        "\n",
        "def acc(pred,label):\n",
        "  # pred = torch.round(pred.squeeze())\n",
        "  # pred = pred.to(\"cpu\").detach().numpy().squeeze()\n",
        "  # label = label.to(\"cpu\").detach().numpy().squeeze()\n",
        "  return accuracy_score(label, pred)\n",
        "\n",
        "def f1(pred, label):\n",
        "  # pred = torch.round(pred.squeeze())\n",
        "  # pred = pred.to(\"cpu\").detach().numpy().squeeze()\n",
        "  # label = label.to(\"cpu\").detach().numpy().squeeze()\n",
        "  pred, label = merge_arrs(pred, label)\n",
        "  return f1_score(label, pred, labels=np.unique(pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RntBfJYw0N"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stop(acc_list):\n",
        "  a = 0\n",
        "  for i in range(len(test_acc), len(test_acc)-10, -1):\n",
        "    if acc_list[i] > acc_list[i-1]: #check if loss is INCREASING (getting worse)\n",
        "      a += 1\n",
        "  return True if a > 7 else False"
      ],
      "metadata": {
        "id": "akLARo9q1QQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794883,
          "user_tz": 240,
          "elapsed": 8,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "a27ca443-ce5d-400b-97c5-84cd5683ffe8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "  test_h = model.init_hidden(batch_size)\n",
        "  test_losses = []\n",
        "  test_f1 = 0.0\n",
        "  preds_all, labels_all = [], []\n",
        "  model.eval()\n",
        "  c = 0\n",
        "  t5 = 0\n",
        "  for ind, (inputs, labels) in enumerate(test_loader):\n",
        "    test_h = tuple([each.data for each in test_h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, test_h = model(inputs, test_h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    output = torch.round(output.squeeze()).to(\"cpu\").detach().numpy().squeeze()\n",
        "    preds_all.append(output)\n",
        "    labels = labels.to(\"cpu\").detach().numpy().squeeze()\n",
        "    labels_all.append(labels)\n",
        "    # f1_score = f1(output,labels)\n",
        "    #add f1 for test predictions\n",
        "    # test_f1 += f1_score\n",
        "    # #check if loss is increasing every 10 posts; save model every 5 posts\n",
        "    # if ind % 5 == 0 and i != 0:\n",
        "    #   save = True\n",
        "    #   c += 1\n",
        "    #   if c == 2:\n",
        "    #     if t5 > test_loss.item():\n",
        "    #       save = False\n",
        "    #     if stop(test_acc):\n",
        "    #       break\n",
        "    #     c = 0\n",
        "    #   if save:\n",
        "    #     torch.save(model.state_dict(), f'/content/saved_model{ind}.pt')\n",
        "    #     if ind != 5: # if ind != 5 AND save == True (meaning if we are saving a new model after 5 iterations, delete the previously saved model)\n",
        "    #       os.remove(f'/content/saved_model{ind-5}.pt')\n",
        "    #   t5 = test_loss.item()\n",
        "  f1_score = f1(preds_all, labels_all)\n",
        "  # return test_losses, test_acc/len(test_loader)\n",
        "  return test_losses, f1_score"
      ],
      "metadata": {
        "id": "enQ85_HsN1Oy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794883,
          "user_tz": 240,
          "elapsed": 7,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f7d2f5-e0d9-41cf-9d4b-f0aec9810792"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0lX9mV3XFvjE",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692846794883,
          "user_tz": 240,
          "elapsed": 6,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8a7831-8c26-478a-df1e-1d4f997fe17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, valid_loader):\n",
        "  clip = 5\n",
        "  epochs = 100\n",
        "  valid_loss_min = np.Inf\n",
        "  # train for some number of epochs\n",
        "  epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "  train_f1_scores, epoch_tr_acc,epoch_vl_acc = [],[],[]\n",
        "  time = 0\n",
        "  for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "    preds_all, labels_all = [], []\n",
        "    for ind, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        #store all predictions/labels in this epoch for the f1_score\n",
        "        output = torch.round(output.squeeze()).to(\"cpu\").detach().numpy().squeeze()\n",
        "        preds_all.append(output)\n",
        "        labels = labels.to(\"cpu\").detach().numpy().squeeze()\n",
        "        labels_all.append(labels)\n",
        "        # calculating accuracy_score\n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        # clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    #f1_score\n",
        "    f1_score = f1(preds_all, labels_all)\n",
        "    #Validation\n",
        "    val_losses, val_acc = test(model, valid_loader)\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader)\n",
        "    epoch_val_acc = val_acc\n",
        "\n",
        "    train_f1_scores.append(float(f'{f1_score:.3f}'))\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "  #return train (acc/loss), val (acc/loss)\n",
        "  return train_f1_scores, max(epoch_tr_acc), min(epoch_tr_loss), max(epoch_vl_acc), epoch_vl_loss   # each list has 100 values (1 per epoch): each value is the average acc/loss after training on entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_t = 0\n",
        "best_model = model\n",
        "best_val_loss = 10000000000\n",
        "#Loss/accuracy results for\n",
        "best_f1, train_losses, train_accs, val_losses, val_accs, test_losses, test_f1_score = [], [], [], [], [], [], []\n",
        "for i in range(10):\n",
        "  a = time.time()\n",
        "  #moving to gpu\n",
        "  model.to(device)\n",
        "  #Train\n",
        "  f1_scores, epoch_tr_acc, epoch_tr_loss, epoch_vl_acc, epoch_vl_losses = train(model, train_loader[i], valid_loader[i])\n",
        "  epoch_vl_loss = min(epoch_vl_losses)\n",
        "  best_f1.append(max(f1_scores))\n",
        "  print(f\"Fold {i+1}'s 100 f1 scores (1 per epoch): {f1_scores}\")\n",
        "  print(f\"Fold {i+1}'s 100 validation losses (1 per epoch): {epoch_vl_losses}\")\n",
        "  train_losses.append(float(f'{epoch_tr_loss:.5f}'))\n",
        "  train_accs.append(float(f'{epoch_tr_acc:.3f}'))\n",
        "  val_losses.append(float(f'{epoch_vl_loss:.5f}'))\n",
        "  val_accs.append(float(f'{epoch_vl_acc:.3f}'))\n",
        "  #you won't need test_losses - this is only used for validations step\n",
        "  test_losses, test_f1 = test(model, test_loader[i])\n",
        "  test_f1_score.append(float(f'{test_f1:.3f}'))\n",
        "  if epoch_vl_loss < best_val_loss:\n",
        "    best_model = model\n",
        "    best_val_loss = epoch_vl_loss\n",
        "  b = time.time()\n",
        "  total_t += b-a\n",
        "  print(f\"Fold {i+1}/10 done\")\n",
        "  model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "\n",
        "torch.save(best_model.state_dict(), '/content/saved_best_model.pt')\n",
        "print(f\"time: {total_t:.3f} seconds\")\n",
        "# below are lists of best accuracies/losses for each fold; AKA max accuracy out of 100 epochs\n",
        "#Train results\n",
        "print(\"Train F1s for each fold:\", best_f1)\n",
        "print(\"Train accuracies:\", train_accs)\n",
        "print(\"Train losses:\", train_losses)\n",
        "#Val results\n",
        "print(\"Val accuracies:\", val_accs)\n",
        "print(\"Val losses:\", val_losses)\n",
        "print(\"Best Val loss:\", best_val_loss)\n",
        "#Test results\n",
        "print(\"Test accuracies:\", test_f1_score)"
      ],
      "metadata": {
        "id": "jP8-H_GVMsr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692851501745,
          "user_tz": 240,
          "elapsed": 4706867,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "a96c096d-bad3-4347-c4a2-8e184f9d27d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0's 100 f1 scores (1 per epoch): [0.0, 0.026, 0.221, 0.422, 0.581, 0.803, 0.85, 0.916, 0.92, 0.976, 0.962, 0.983, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Fold 0's 100 validation losses (1 per epoch): [0.21271194517612457, 0.1909072779946857, 0.16621250576443142, 0.16229194154342017, 0.1584204381538762, 0.1812002366512186, 0.20708582808987963, 0.2085589555402597, 0.1911847550008032, 0.2638282231572602, 0.2595466644399696, 0.2709993583395974, 0.3101692560594529, 0.327099537198794, 0.33640015231665327, 0.34041438377527, 0.35489711589697337, 0.35727751058968554, 0.36437802192469665, 0.3719286559563544, 0.37852607294917107, 0.38349769316361443, 0.38684387994312097, 0.3958637908120484, 0.4063650707850886, 0.40454929479084156, 0.41068203817768434, 0.40898613246463356, 0.4175324219773352, 0.41576466389233246, 0.4224427746747905, 0.42452732052535996, 0.4263810370325195, 0.43014237380379605, 0.43562767585131545, 0.4428504239250388, 0.4371068701603033, 0.44083138141367173, 0.4486707626104665, 0.44801829038705265, 0.45961298483032603, 0.45299871055168395, 0.4562690899825712, 0.464107261154393, 0.46252122909613214, 0.46450912980718484, 0.46585985815285214, 0.47091320814150905, 0.47587302796699127, 0.482761002784375, 0.4867497095983708, 0.4782514846980285, 0.4808248174764837, 0.49124219414888004, 0.49319550817042707, 0.49046463875048274, 0.49041880523630726, 0.49634159388289034, 0.49537609325933674, 0.5038782129200475, 0.5012647340902023, 0.5051720844090192, 0.5052412841334457, 0.5168555592396792, 0.5160719215895774, 0.5084704747864381, 0.5108379625641027, 0.5213044228063445, 0.5178471914389067, 0.5147944710878013, 0.5271551820656492, 0.5235183279855783, 0.5247274641911177, 0.5289522433134203, 0.5309053214861907, 0.5305464131667732, 0.5335407389453495, 0.5336057113276588, 0.5437037317903446, 0.541837238575719, 0.5440399059346722, 0.5428555052307931, 0.5478809283732617, 0.5485072169104923, 0.5475057117218967, 0.5477404652564373, 0.550267128414109, 0.554989093217993, 0.5546823800822204, 0.5647209690295212, 0.5741100934143586, 0.5696714993100613, 0.5733563084924955, 0.5728492245535663, 0.5654516540784648, 0.5728543404407979, 0.5699809016724784, 0.5706475781049534, 0.5912099112239149, 0.5752453605333964]\n",
            "Fold 1/10 done\n",
            "Fold 1's 100 f1 scores (1 per epoch): [0.1, 0.1, 0.1, 0.098, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.101, 0.1, 0.1, 0.099, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.099, 0.1, 0.097, 0.1, 0.1, 0.101, 0.099, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.1, 0.099, 0.101, 0.099, 0.099, 0.1, 0.1, 0.1, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.101, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.098, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.099, 0.1]\n",
            "Fold 1's 100 validation losses (1 per epoch): [0.7116755975617303, 0.7116755776935153, 0.7116755776935153, 0.7116755776935153, 0.7116755975617303, 0.7116756041844686, 0.711675590938992, 0.7116756041844686, 0.7116755975617303, 0.7116756041844686, 0.7116755975617303, 0.7116755975617303, 0.7116755776935153, 0.7116755843162537, 0.7116756041844686, 0.711675590938992, 0.7116756041844686, 0.7116755710707771, 0.7116755975617303, 0.7116756041844686, 0.7116755843162537, 0.711675590938992, 0.711675590938992, 0.7116755843162537, 0.7116756041844686, 0.7116756108072069, 0.7116755776935153, 0.7116755843162537, 0.7116756240526835, 0.7116756041844686, 0.7116755975617303, 0.7116755975617303, 0.7116755975617303, 0.7116755843162537, 0.7116755975617303, 0.7116755975617303, 0.7116755975617303, 0.711675590938992, 0.7116755843162537, 0.7116755975617303, 0.7116755843162537, 0.7116755776935153, 0.7116755975617303, 0.7116755975617303, 0.7116755975617303, 0.7116755776935153, 0.7116755975617303, 0.711675590938992, 0.7116755644480387, 0.7116755975617303, 0.711675590938992, 0.7116755975617303, 0.7116755776935153, 0.7116755975617303, 0.711675590938992, 0.7116755710707771, 0.7116755975617303, 0.7116756041844686, 0.7116755975617303, 0.711675590938992, 0.7116755776935153, 0.7116755776935153, 0.7116756108072069, 0.7116756108072069, 0.7116755975617303, 0.7116756108072069, 0.711675590938992, 0.7116756041844686, 0.7116755776935153, 0.7116755843162537, 0.7116755776935153, 0.7116755975617303, 0.7116755843162537, 0.7116755843162537, 0.7116755843162537, 0.7116755843162537, 0.7116755975617303, 0.711675590938992, 0.7116755975617303, 0.7116755843162537, 0.7116755975617303, 0.711675590938992, 0.7116756108072069, 0.711675590938992, 0.7116755710707771, 0.7116755776935153, 0.711675590938992, 0.7116755975617303, 0.7116755975617303, 0.7116756041844686, 0.7116755776935153, 0.7116756041844686, 0.7116755843162537, 0.711675590938992, 0.7116755975617303, 0.711675590938992, 0.7116756041844686, 0.7116755843162537, 0.7116755776935153, 0.7116756041844686]\n",
            "Fold 2/10 done\n",
            "Fold 2's 100 f1 scores (1 per epoch): [0.096, 0.096, 0.099, 0.099, 0.097, 0.1, 0.098, 0.096, 0.099, 0.101, 0.096, 0.101, 0.099, 0.096, 0.098, 0.098, 0.097, 0.097, 0.098, 0.101, 0.098, 0.098, 0.097, 0.1, 0.098, 0.097, 0.1, 0.1, 0.098, 0.097, 0.097, 0.098, 0.099, 0.098, 0.098, 0.096, 0.099, 0.097, 0.098, 0.096, 0.098, 0.1, 0.096, 0.096, 0.099, 0.095, 0.098, 0.098, 0.099, 0.097, 0.1, 0.096, 0.096, 0.094, 0.098, 0.098, 0.097, 0.099, 0.101, 0.097, 0.098, 0.097, 0.098, 0.098, 0.099, 0.099, 0.094, 0.1, 0.098, 0.098, 0.1, 0.098, 0.099, 0.098, 0.1, 0.099, 0.098, 0.097, 0.095, 0.096, 0.096, 0.1, 0.1, 0.098, 0.096, 0.095, 0.095, 0.099, 0.098, 0.097, 0.096, 0.098, 0.098, 0.1, 0.099, 0.098, 0.1, 0.097, 0.098, 0.098]\n",
            "Fold 2's 100 validation losses (1 per epoch): [0.7062276535563998, 0.7062276932928298, 0.7062276668018765, 0.7062276535563998, 0.7062276734246148, 0.7062276601791382, 0.7062276734246148, 0.7062276734246148, 0.7062276601791382, 0.7062276668018765, 0.7062276734246148, 0.7062276601791382, 0.7062276469336616, 0.7062276668018765, 0.7062276800473531, 0.7062276601791382, 0.7062276668018765, 0.7062276535563998, 0.7062276800473531, 0.706227699915568, 0.7062276668018765, 0.7062276535563998, 0.7062276535563998, 0.7062276668018765, 0.7062276668018765, 0.7062276668018765, 0.7062276535563998, 0.7062276668018765, 0.7062276535563998, 0.7062276734246148, 0.7062276734246148, 0.7062276668018765, 0.7062276601791382, 0.7062276668018765, 0.7062276734246148, 0.7062276601791382, 0.7062276734246148, 0.7062276535563998, 0.7062276668018765, 0.7062276734246148, 0.7062276601791382, 0.7062276734246148, 0.7062276668018765, 0.7062276668018765, 0.7062276734246148, 0.7062276535563998, 0.7062276668018765, 0.7062276668018765, 0.7062276601791382, 0.7062276469336616, 0.7062276866700914, 0.7062276668018765, 0.7062276734246148, 0.7062276535563998, 0.7062276734246148, 0.7062276734246148, 0.7062276668018765, 0.7062276535563998, 0.7062276866700914, 0.7062276601791382, 0.7062276601791382, 0.7062276668018765, 0.7062276601791382, 0.7062276668018765, 0.7062276734246148, 0.7062276734246148, 0.7062276866700914, 0.7062276734246148, 0.7062276668018765, 0.7062276668018765, 0.7062276668018765, 0.7062276734246148, 0.7062276535563998, 0.7062276800473531, 0.7062276668018765, 0.7062276734246148, 0.7062276601791382, 0.7062276601791382, 0.7062276601791382, 0.7062276601791382, 0.7062276734246148, 0.7062276800473531, 0.7062276668018765, 0.7062276734246148, 0.7062276800473531, 0.7062276668018765, 0.7062276668018765, 0.7062276668018765, 0.7062276866700914, 0.7062276469336616, 0.7062276469336616, 0.7062276601791382, 0.7062276800473531, 0.7062276535563998, 0.7062276734246148, 0.7062276601791382, 0.7062276601791382, 0.7062276535563998, 0.7062276734246148, 0.7062276800473531]\n",
            "Fold 3/10 done\n",
            "Fold 3's 100 f1 scores (1 per epoch): [0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.013, 0.013, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.013, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013]\n",
            "Fold 3's 100 validation losses (1 per epoch): [0.6719325515958998, 0.6719325648413764, 0.6719325582186381, 0.6719325449731615, 0.6719325847095914, 0.6719325714641147, 0.671932578086853, 0.6719325648413764, 0.6719325714641147, 0.6719325648413764, 0.6719325847095914, 0.6719325515958998, 0.6719325648413764, 0.6719325515958998, 0.6719325648413764, 0.6719325582186381, 0.6719325648413764, 0.6719325648413764, 0.671932578086853, 0.6719325515958998, 0.6719325714641147, 0.6719325582186381, 0.6719325714641147, 0.6719325648413764, 0.6719325714641147, 0.6719325582186381, 0.6719325714641147, 0.671932578086853, 0.6719325449731615, 0.6719325648413764, 0.6719325648413764, 0.671932578086853, 0.6719325847095914, 0.671932578086853, 0.6719325648413764, 0.6719325648413764, 0.6719325648413764, 0.6719325648413764, 0.6719325515958998, 0.6719325714641147, 0.6719325449731615, 0.671932578086853, 0.6719325714641147, 0.6719325714641147, 0.6719325582186381, 0.671932578086853, 0.6719325582186381, 0.6719325648413764, 0.6719325648413764, 0.6719325449731615, 0.6719325515958998, 0.6719325515958998, 0.6719325714641147, 0.6719325648413764, 0.6719325515958998, 0.6719325648413764, 0.6719325449731615, 0.6719325648413764, 0.6719325714641147, 0.6719325515958998, 0.6719325714641147, 0.6719325582186381, 0.6719325648413764, 0.6719325582186381, 0.6719325714641147, 0.6719325648413764, 0.6719325648413764, 0.6719325515958998, 0.6719325648413764, 0.6719325582186381, 0.6719325515958998, 0.671932578086853, 0.6719325648413764, 0.6719325714641147, 0.6719325648413764, 0.6719325648413764, 0.6719325648413764, 0.6719325582186381, 0.6719325648413764, 0.6719325582186381, 0.6719325648413764, 0.6719325714641147, 0.6719325648413764, 0.6719325582186381, 0.6719325714641147, 0.6719325515958998, 0.6719325714641147, 0.6719325582186381, 0.6719325714641147, 0.6719325515958998, 0.6719325383504232, 0.6719325714641147, 0.671932578086853, 0.6719325515958998, 0.6719325648413764, 0.6719325714641147, 0.6719325582186381, 0.6719325648413764, 0.671932578086853, 0.6719325449731615]\n",
            "Fold 4/10 done\n",
            "Fold 4's 100 f1 scores (1 per epoch): [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "Fold 4's 100 validation losses (1 per epoch): [0.7218217319912381, 0.7218217253684998, 0.7218217319912381, 0.7218217319912381, 0.7218217452367147, 0.7218217452367147, 0.7218217452367147, 0.7218217319912381, 0.7218217319912381, 0.721821751859453, 0.7218217319912381, 0.7218217319912381, 0.7218217584821913, 0.7218217386139764, 0.7218217386139764, 0.7218217386139764, 0.7218217452367147, 0.721821751859453, 0.7218217386139764, 0.721821751859453, 0.7218217253684998, 0.7218217187457614, 0.7218217386139764, 0.7218217319912381, 0.7218217319912381, 0.7218217452367147, 0.7218217386139764, 0.7218217452367147, 0.7218217319912381, 0.7218217452367147, 0.7218217253684998, 0.7218217253684998, 0.7218217386139764, 0.7218217253684998, 0.7218217319912381, 0.721821751859453, 0.7218217319912381, 0.7218217386139764, 0.7218217055002848, 0.721821751859453, 0.7218217651049296, 0.7218217386139764, 0.7218217253684998, 0.7218217386139764, 0.7218217386139764, 0.721821751859453, 0.7218217386139764, 0.7218217319912381, 0.7218217452367147, 0.7218217386139764, 0.7218217253684998, 0.721821751859453, 0.7218217319912381, 0.7218217452367147, 0.7218217452367147, 0.7218217386139764, 0.7218217187457614, 0.7218217253684998, 0.7218217386139764, 0.7218217452367147, 0.7218217452367147, 0.721821751859453, 0.7218217253684998, 0.7218217386139764, 0.7218217319912381, 0.7218217452367147, 0.721821751859453, 0.7218217386139764, 0.721821751859453, 0.7218217386139764, 0.721821751859453, 0.721821751859453, 0.7218217452367147, 0.7218217386139764, 0.7218217386139764, 0.7218217386139764, 0.7218217452367147, 0.7218217452367147, 0.721821751859453, 0.7218217452367147, 0.7218217121230232, 0.7218217452367147, 0.7218217386139764, 0.7218217452367147, 0.7218217452367147, 0.7218217386139764, 0.7218217319912381, 0.7218217319912381, 0.7218217319912381, 0.7218217253684998, 0.7218217452367147, 0.7218217319912381, 0.7218217452367147, 0.7218217452367147, 0.7218217319912381, 0.7218217319912381, 0.7218217187457614, 0.7218217319912381, 0.721821751859453, 0.7218217452367147]\n",
            "Fold 5/10 done\n",
            "Fold 5's 100 f1 scores (1 per epoch): [0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099]\n",
            "Fold 5's 100 validation losses (1 per epoch): [0.7282044225268893, 0.7282044291496277, 0.728204435772366, 0.728204435772366, 0.728204435772366, 0.7282044291496277, 0.728204435772366, 0.7282044291496277, 0.728204435772366, 0.728204435772366, 0.7282044225268893, 0.728204435772366, 0.7282044291496277, 0.7282044159041511, 0.7282044159041511, 0.7282044225268893, 0.7282044159041511, 0.728204435772366, 0.728204435772366, 0.728204435772366, 0.7282044291496277, 0.7282044423951043, 0.728204435772366, 0.728204435772366, 0.7282044225268893, 0.7282044423951043, 0.7282044092814127, 0.7282044092814127, 0.728204435772366, 0.7282044490178426, 0.728204435772366, 0.7282044291496277, 0.728204435772366, 0.728204435772366, 0.7282044291496277, 0.728204435772366, 0.7282044291496277, 0.7282044225268893, 0.7282044291496277, 0.7282044225268893, 0.7282044159041511, 0.7282044423951043, 0.7282044026586745, 0.7282044423951043, 0.7282044225268893, 0.7282044556405809, 0.728204435772366, 0.7282044291496277, 0.7282044159041511, 0.7282044291496277, 0.728204435772366, 0.728204435772366, 0.7282044490178426, 0.7282044423951043, 0.7282044159041511, 0.728204435772366, 0.728204435772366, 0.728204435772366, 0.7282044291496277, 0.7282044291496277, 0.7282044225268893, 0.7282044225268893, 0.728204435772366, 0.7282044159041511, 0.728204435772366, 0.7282044225268893, 0.728204435772366, 0.728204435772366, 0.7282044225268893, 0.7282044225268893, 0.7282044423951043, 0.7282044159041511, 0.7282044423951043, 0.7282044291496277, 0.7282044291496277, 0.7282044291496277, 0.7282044225268893, 0.7282044291496277, 0.7282044291496277, 0.7282044159041511, 0.728204435772366, 0.7282044423951043, 0.7282044159041511, 0.7282044291496277, 0.7282044423951043, 0.7282044423951043, 0.728204435772366, 0.7282044225268893, 0.728204435772366, 0.728204435772366, 0.7282044423951043, 0.7282044225268893, 0.7282044423951043, 0.728204435772366, 0.728204435772366, 0.7282044291496277, 0.728204435772366, 0.7282044225268893, 0.728204435772366, 0.7282044423951043]\n",
            "Fold 6/10 done\n",
            "Fold 6's 100 f1 scores (1 per epoch): [0.083, 0.092, 0.088, 0.085, 0.078, 0.077, 0.082, 0.083, 0.081, 0.086, 0.099, 0.086, 0.083, 0.069, 0.082, 0.078, 0.096, 0.067, 0.091, 0.085, 0.082, 0.085, 0.088, 0.093, 0.087, 0.091, 0.098, 0.083, 0.087, 0.096, 0.078, 0.09, 0.078, 0.079, 0.101, 0.086, 0.086, 0.091, 0.074, 0.085, 0.086, 0.08, 0.078, 0.087, 0.086, 0.084, 0.087, 0.077, 0.088, 0.074, 0.072, 0.082, 0.077, 0.075, 0.095, 0.084, 0.087, 0.091, 0.085, 0.086, 0.073, 0.071, 0.084, 0.082, 0.095, 0.076, 0.078, 0.083, 0.074, 0.075, 0.081, 0.093, 0.078, 0.096, 0.109, 0.094, 0.098, 0.074, 0.085, 0.077, 0.1, 0.088, 0.097, 0.077, 0.08, 0.084, 0.085, 0.072, 0.087, 0.086, 0.086, 0.091, 0.089, 0.076, 0.081, 0.074, 0.075, 0.087, 0.073, 0.082]\n",
            "Fold 6's 100 validation losses (1 per epoch): [0.6921917862362332, 0.6921917663680183, 0.6921917796134949, 0.6921917796134949, 0.6921917663680183, 0.6921917663680183, 0.6921917729907565, 0.6921917729907565, 0.6921917796134949, 0.6921917796134949, 0.6921917994817098, 0.6921917729907565, 0.6921917663680183, 0.6921917729907565, 0.6921917597452799, 0.6921917928589715, 0.6921917796134949, 0.6921917796134949, 0.6921917796134949, 0.6921917862362332, 0.6921917862362332, 0.6921917663680183, 0.6921917796134949, 0.6921917928589715, 0.6921917729907565, 0.6921917928589715, 0.6921917729907565, 0.6921917663680183, 0.6921917729907565, 0.6921917862362332, 0.6921917729907565, 0.6921917729907565, 0.6921917796134949, 0.6921917796134949, 0.6921917663680183, 0.6921917597452799, 0.6921917994817098, 0.6921917796134949, 0.6921917862362332, 0.6921917597452799, 0.6921917729907565, 0.6921917663680183, 0.6921917663680183, 0.6921917663680183, 0.6921917663680183, 0.6921917729907565, 0.6921917729907565, 0.6921917928589715, 0.6921917729907565, 0.6921917862362332, 0.6921917796134949, 0.6921917729907565, 0.6921917796134949, 0.6921917796134949, 0.6921917796134949, 0.6921917729907565, 0.6921917729907565, 0.6921917928589715, 0.6921917729907565, 0.6921917729907565, 0.6921917729907565, 0.6921917862362332, 0.6921917796134949, 0.6921917597452799, 0.6921917862362332, 0.6921917729907565, 0.6921917862362332, 0.6921917796134949, 0.6921917729907565, 0.6921917729907565, 0.6921917796134949, 0.6921917729907565, 0.6921917729907565, 0.6921917796134949, 0.6921917862362332, 0.6921917729907565, 0.6921917663680183, 0.6921917597452799, 0.6921917729907565, 0.6921917862362332, 0.6921917862362332, 0.6921917663680183, 0.6921917796134949, 0.6921917796134949, 0.6921917928589715, 0.6921917729907565, 0.6921917928589715, 0.6921917729907565, 0.6921917663680183, 0.6921917663680183, 0.6921917862362332, 0.6921917663680183, 0.6921917663680183, 0.6921917663680183, 0.6921917928589715, 0.6921917729907565, 0.6921917663680183, 0.6921917663680183, 0.6921917729907565, 0.6921917729907565]\n",
            "Fold 7/10 done\n",
            "Fold 7's 100 f1 scores (1 per epoch): [0.066, 0.051, 0.059, 0.048, 0.054, 0.044, 0.055, 0.075, 0.059, 0.07, 0.048, 0.044, 0.036, 0.069, 0.042, 0.039, 0.034, 0.065, 0.067, 0.077, 0.043, 0.068, 0.076, 0.058, 0.049, 0.061, 0.047, 0.054, 0.051, 0.058, 0.052, 0.075, 0.049, 0.091, 0.053, 0.061, 0.062, 0.043, 0.068, 0.065, 0.055, 0.066, 0.044, 0.047, 0.05, 0.029, 0.06, 0.07, 0.062, 0.041, 0.06, 0.048, 0.044, 0.056, 0.01, 0.071, 0.061, 0.103, 0.054, 0.041, 0.052, 0.055, 0.054, 0.038, 0.061, 0.055, 0.061, 0.042, 0.047, 0.064, 0.046, 0.042, 0.055, 0.046, 0.047, 0.05, 0.035, 0.055, 0.048, 0.028, 0.059, 0.046, 0.038, 0.051, 0.029, 0.081, 0.07, 0.048, 0.049, 0.064, 0.04, 0.075, 0.039, 0.067, 0.034, 0.029, 0.031, 0.069, 0.073, 0.033]\n",
            "Fold 7's 100 validation losses (1 per epoch): [0.6839902003606161, 0.6839901871151395, 0.6839902069833543, 0.6839901804924011, 0.6839902003606161, 0.6839901937378777, 0.6839901804924011, 0.6839902202288309, 0.6839902069833543, 0.6839901937378777, 0.6839901804924011, 0.6839902003606161, 0.6839901871151395, 0.6839901804924011, 0.6839902003606161, 0.6839902003606161, 0.6839902069833543, 0.6839902003606161, 0.6839901672469245, 0.6839902069833543, 0.6839901937378777, 0.6839902069833543, 0.6839902003606161, 0.6839901871151395, 0.6839901937378777, 0.6839902069833543, 0.6839901937378777, 0.6839901937378777, 0.6839902003606161, 0.6839902069833543, 0.6839902069833543, 0.6839902069833543, 0.6839902136060927, 0.6839902069833543, 0.6839902069833543, 0.6839902069833543, 0.6839901738696628, 0.6839901937378777, 0.6839902069833543, 0.6839902069833543, 0.6839902003606161, 0.6839901804924011, 0.6839901871151395, 0.6839901937378777, 0.6839902003606161, 0.6839901871151395, 0.6839902136060927, 0.6839902003606161, 0.6839901937378777, 0.6839901937378777, 0.6839902136060927, 0.6839902136060927, 0.6839901871151395, 0.6839901937378777, 0.6839902268515693, 0.6839902136060927, 0.6839901672469245, 0.6839902003606161, 0.6839902003606161, 0.6839902069833543, 0.6839902069833543, 0.6839902069833543, 0.6839902003606161, 0.6839901871151395, 0.6839902069833543, 0.6839901937378777, 0.6839901871151395, 0.6839902202288309, 0.6839901937378777, 0.6839902003606161, 0.6839902003606161, 0.6839902069833543, 0.6839902003606161, 0.6839901871151395, 0.6839901871151395, 0.6839901804924011, 0.6839902069833543, 0.6839901804924011, 0.6839902136060927, 0.6839902069833543, 0.6839902069833543, 0.6839901804924011, 0.6839901937378777, 0.6839902003606161, 0.6839901937378777, 0.6839901937378777, 0.6839901937378777, 0.6839901937378777, 0.6839902136060927, 0.6839902003606161, 0.6839901937378777, 0.6839902003606161, 0.6839901871151395, 0.6839902003606161, 0.6839901937378777, 0.6839902003606161, 0.6839901937378777, 0.6839901804924011, 0.6839902069833543, 0.6839901937378777]\n",
            "Fold 8/10 done\n",
            "Fold 8's 100 f1 scores (1 per epoch): [0.09, 0.089, 0.091, 0.099, 0.098, 0.094, 0.094, 0.099, 0.091, 0.096, 0.094, 0.098, 0.095, 0.094, 0.097, 0.096, 0.095, 0.095, 0.099, 0.098, 0.094, 0.102, 0.097, 0.096, 0.096, 0.098, 0.096, 0.096, 0.09, 0.099, 0.097, 0.099, 0.096, 0.095, 0.098, 0.094, 0.095, 0.097, 0.1, 0.101, 0.099, 0.099, 0.096, 0.094, 0.098, 0.096, 0.098, 0.098, 0.094, 0.095, 0.09, 0.099, 0.099, 0.098, 0.098, 0.1, 0.097, 0.093, 0.097, 0.099, 0.096, 0.088, 0.097, 0.095, 0.096, 0.097, 0.094, 0.093, 0.091, 0.1, 0.094, 0.096, 0.098, 0.1, 0.1, 0.096, 0.094, 0.092, 0.094, 0.093, 0.094, 0.098, 0.1, 0.093, 0.101, 0.096, 0.097, 0.087, 0.095, 0.099, 0.099, 0.096, 0.096, 0.095, 0.087, 0.092, 0.094, 0.097, 0.095, 0.098]\n",
            "Fold 8's 100 validation losses (1 per epoch): [0.700077924463484, 0.7000779443316989, 0.7000779443316989, 0.700077924463484, 0.700077924463484, 0.7000779509544373, 0.7000779310862223, 0.700077924463484, 0.7000779377089607, 0.7000779377089607, 0.7000779178407457, 0.7000779310862223, 0.7000779178407457, 0.7000779377089607, 0.7000779443316989, 0.700077924463484, 0.7000779377089607, 0.7000779575771756, 0.700077924463484, 0.7000779310862223, 0.7000779310862223, 0.7000779377089607, 0.7000779178407457, 0.7000779641999139, 0.7000779509544373, 0.7000779178407457, 0.7000779310862223, 0.7000779310862223, 0.7000779377089607, 0.7000779377089607, 0.7000779310862223, 0.7000779377089607, 0.7000779178407457, 0.700077924463484, 0.7000779377089607, 0.7000779575771756, 0.7000779443316989, 0.7000779310862223, 0.7000779310862223, 0.7000779178407457, 0.7000779112180074, 0.7000779178407457, 0.7000779310862223, 0.7000779310862223, 0.7000779377089607, 0.7000779377089607, 0.7000779310862223, 0.7000779377089607, 0.7000779178407457, 0.7000779377089607, 0.7000779377089607, 0.700077924463484, 0.7000779112180074, 0.7000779310862223, 0.7000779310862223, 0.7000779575771756, 0.700077924463484, 0.7000779377089607, 0.7000779310862223, 0.7000779377089607, 0.7000779377089607, 0.700077924463484, 0.7000779377089607, 0.7000779310862223, 0.7000779310862223, 0.7000779178407457, 0.7000779509544373, 0.700077924463484, 0.7000779377089607, 0.7000779377089607, 0.7000779377089607, 0.7000779377089607, 0.7000779112180074, 0.7000779310862223, 0.7000779112180074, 0.7000779443316989, 0.7000779443316989, 0.7000779178407457, 0.7000779310862223, 0.7000779377089607, 0.7000779310862223, 0.700077924463484, 0.700077924463484, 0.7000779377089607, 0.7000779509544373, 0.700077924463484, 0.7000779443316989, 0.7000779443316989, 0.7000779377089607, 0.700077924463484, 0.7000779377089607, 0.7000779443316989, 0.7000779310862223, 0.700077924463484, 0.7000779377089607, 0.7000779310862223, 0.7000779509544373, 0.7000779310862223, 0.700077924463484, 0.7000779310862223]\n",
            "Fold 9/10 done\n",
            "Fold 9's 100 f1 scores (1 per epoch): [0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099]\n",
            "Fold 9's 100 validation losses (1 per epoch): [0.7252248989211189, 0.7252248724301656, 0.7252248724301656, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248922983805, 0.7252248724301656, 0.7252248724301656, 0.7252248856756423, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248856756423, 0.7252248790529039, 0.7252248790529039, 0.7252248790529039, 0.725224859184689, 0.7252248790529039, 0.7252248724301656, 0.7252248790529039, 0.7252248790529039, 0.7252248856756423, 0.725224859184689, 0.7252248724301656, 0.7252248724301656, 0.7252248856756423, 0.7252248724301656, 0.7252248724301656, 0.7252248856756423, 0.7252248790529039, 0.7252248856756423, 0.7252248724301656, 0.7252248724301656, 0.7252248922983805, 0.7252248724301656, 0.7252248724301656, 0.7252248724301656, 0.725224859184689, 0.7252248658074273, 0.7252248658074273, 0.7252248856756423, 0.7252248790529039, 0.7252248724301656, 0.725224859184689, 0.7252248724301656, 0.725224859184689, 0.725224859184689, 0.7252248790529039, 0.7252248790529039, 0.7252248658074273, 0.7252248658074273, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248922983805, 0.7252248856756423, 0.7252248724301656, 0.7252248790529039, 0.725224859184689, 0.7252248724301656, 0.7252248790529039, 0.7252248790529039, 0.725224859184689, 0.7252248790529039, 0.7252248724301656, 0.7252248724301656, 0.7252248658074273, 0.7252248790529039, 0.7252248658074273, 0.7252248856756423, 0.7252248790529039, 0.7252248790529039, 0.725224859184689, 0.7252248724301656, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248724301656, 0.7252248790529039, 0.7252248724301656, 0.7252248856756423, 0.7252248724301656, 0.7252248658074273, 0.7252248724301656, 0.7252248724301656, 0.7252248922983805, 0.7252248790529039, 0.7252248658074273, 0.7252248724301656, 0.7252248658074273, 0.7252248658074273, 0.7252248724301656, 0.7252248856756423, 0.7252248856756423, 0.7252248856756423, 0.7252248790529039, 0.7252248724301656]\n",
            "Fold 10/10 done\n",
            "time: 4706.817 seconds\n",
            "Train F1s for each fold: [1.0, 0.101, 0.101, 0.026, 0.1, 0.099, 0.109, 0.103, 0.102, 0.099]\n",
            "Train accuracies: [1.0, 0.06, 0.106, 0.948, 0.054, 0.053, 0.579, 0.867, 0.207, 0.053]\n",
            "Train losses: [0.0, 0.71141, 0.70541, 0.67116, 0.72112, 0.72867, 0.69177, 0.68335, 0.70004, 0.72488]\n",
            "Val accuracies: [0.562, 0.097, 0.102, 0.0, 0.097, 0.102, 0.064, 0.174, 0.095, 0.097]\n",
            "Val losses: [0.59121, 0.71168, 0.70623, 0.67193, 0.72182, 0.7282, 0.69219, 0.68399, 0.70008, 0.72522]\n",
            "Best Val loss: 0.5912099112239149\n",
            "Test accuracies: [0.267, 0.098, 0.098, 0.0, 0.098, 0.098, 0.076, 0.074, 0.103, 0.103]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"time: {total_t:.3f} seconds\")\n",
        "# below are lists of best accuracies/losses for each fold; AKA max accuracy out of 100 epochs\n",
        "#Train results\n",
        "print(\"Train F1s for each fold:\", best_f1)\n",
        "print(\"Train accuracies:\", train_accs)\n",
        "print(\"Train losses:\", train_losses)\n",
        "#Val results\n",
        "print(\"Val F1s for each fold:\", val_accs)\n",
        "print(\"Val losses:\", val_losses)\n",
        "print(\"Best Val loss:\", best_val_loss)\n",
        "#Test results\n",
        "print(\"Test accuracies:\", test_f1_score)\n",
        "print(min(epoch_vl_losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kSPivpzbypX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692851820432,
          "user_tz": 240,
          "elapsed": 4,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "1d57a655-f024-447e-9450-02bb750ad470"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4706.817 seconds\n",
            "Train F1s for each fold: [1.0, 0.101, 0.101, 0.026, 0.1, 0.099, 0.109, 0.103, 0.102, 0.099]\n",
            "Train accuracies: [1.0, 0.06, 0.106, 0.948, 0.054, 0.053, 0.579, 0.867, 0.207, 0.053]\n",
            "Train losses: [0.0, 0.71141, 0.70541, 0.67116, 0.72112, 0.72867, 0.69177, 0.68335, 0.70004, 0.72488]\n",
            "Val F1s for each fold: [0.562, 0.097, 0.102, 0.0, 0.097, 0.102, 0.064, 0.174, 0.095, 0.097]\n",
            "Val losses: [0.59121, 0.71168, 0.70623, 0.67193, 0.72182, 0.7282, 0.69219, 0.68399, 0.70008, 0.72522]\n",
            "Best Val loss: 0.5912099112239149\n",
            "Test accuracies: [0.267, 0.098, 0.098, 0.0, 0.098, 0.098, 0.076, 0.074, 0.103, 0.103]\n",
            "0.725224859184689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E59uS4HIFyHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1692851502006,
          "user_tz": 240,
          "elapsed": 267,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "3896f885-2a44-4dcf-cc96-0e34109d6985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ive been meaning to make a post on here for a while, but Im happy to finally say Ive been vape free for 61 days now! Id only been previously vaping for about a year so my case isnt as bad as some, but I hated how attached I was to my vape and wanted to quit. I knew it wasnt good for my health and since Im into some singing and vocal performance, I knew I needed to stop soon. \n",
            "\n",
            "Family and friends were encouraging me for a few months but it didnt help out too much. Sometimes Id try to stop for a day or two, but Id see my vape and get right back to it. But, one day I thought Im not gonna let this control me. Instead of I think Im going to quit, I said I WILL quit vaping. \n",
            "\n",
            "The first couple days it was terrible. By day 3 I got so close to going to buy one, but I thought back to myself and kept going. I will say though, once I made it to 1 week, it got a lot easier from there. I still get headaches and urges even now, but they normally go away after a few minutes. \n",
            "\n",
            "For anyone who sees this trying to quit, YOU CAN DO IT! I BELIEVE IN YOU! I never thought I would quit this soon, but if you really commit and do it for yourself, you got this. \n",
            "\n",
            "Some things that helped me were day 1, I took ALL my vapes to my school and threw everything out in the trash there. Not having it in my trash at home took out the option of reaching back for it later. Having them out of sight helped out tremendously. \n",
            "\n",
            "Also the truth initiative text line helped out a lot too. If youre like me and dont feel comfortable bringing your addiction up to anyone, having encouraging texts come in felt like someone was pushing me to keep going. I still use it even now if I ever get urges. If u want it, text DITCHVAPE to 88709\n",
            "\n",
            "Stay strong everyone! I love you all\n",
            "======================================================================\n",
            "Actual sentiment is  : 4270   NaN\n",
            "4271   NaN\n",
            "Name: labels, dtype: float64\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e2e3bdaa8f56>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Actual sentiment is  : {df[\"labels\"][4270:4272]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4270\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# status = \"positive\" if pro > 0.5 else \"negative\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e2e3bdaa8f56>\u001b[0m in \u001b[0;36mpredict_text\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c2dbd75c0563>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# embeddings and lstm_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: B x S x Feature   since batch = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(embeds.shape)  #[50, 500, 1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "def predict_text(texts):\n",
        "  l = []\n",
        "  for text in texts:\n",
        "    word_seq = np.array([vocab[preprocess_string(word)] for word in text.split()\n",
        "                      if preprocess_string(word) in vocab.keys()])\n",
        "    word_seq = np.expand_dims(word_seq,axis=0)\n",
        "    pad =  torch.from_numpy(padding_(word_seq,500))\n",
        "    inputs = pad.to(device)\n",
        "    batch_size = 1\n",
        "    h = model.init_hidden(batch_size)\n",
        "    h = tuple([each.data for each in h])\n",
        "    output, h = model(inputs, h)\n",
        "    l.append(output.item())\n",
        "  return l\n",
        "\n",
        "index = 4274\n",
        "print(df['selftext'][4271])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df[\"labels\"][4270:4272]}')\n",
        "print('='*70)\n",
        "pro = predict_text(df['selftext'][4270:index])\n",
        "print(pro)\n",
        "# status = \"positive\" if pro > 0.5 else \"negative\"\n",
        "# pro = (1 - pro) if status == \"negative\" else pro\n",
        "# print(f'Predicted sentiment is {status} with a probability of {pro}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "16cpcIHefCClZJFjAqfI462XtRnKiFin3",
          "timestamp": 1692752194967
        },
        {
          "file_id": "https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb",
          "timestamp": 1692203859921
        }
      ],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
