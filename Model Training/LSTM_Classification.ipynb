{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joongeun/Internship/blob/main/LSTM_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4giRzM7NtHJ",
        "outputId": "a1622517-99ae-4977-89a4-67014bf5acb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from matplotlib import pyplot\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMV27R008-Jx",
        "outputId": "8e53065b-9cf5-4880-8a6e-ed4837112474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwJrQFQgN_BE",
        "outputId": "5ee6bd65-a315-487d-a658-51accda228d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    0.522571\n",
              "4.0    0.345714\n",
              "2.0    0.052286\n",
              "3.0    0.043429\n",
              "0.0    0.028286\n",
              "5.0    0.007714\n",
              "Name: labels, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/Labeled Posts - preprocessed_csv.csv\")\n",
        "# check class distribution\n",
        "df['labels'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "outputs": [],
      "source": [
        "X,y = df['selftext'][0:3500].values,df['labels'][0:3500].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4K6OKQSZji"
      },
      "source": [
        "# Preprocess/Tokenization\n",
        "(Convert text into numbers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R6hRrn3JDf0D"
      },
      "outputs": [],
      "source": [
        "def preprocess_string(s):\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "    split = re.findall(r\"[\\w']+|[!?]\", s)\n",
        "    i = 0\n",
        "    while i < len(split)-1:\n",
        "      if split[i] == split[i+1]:\n",
        "        del split[i+1]\n",
        "        i-=1\n",
        "      i+= 1\n",
        "    # Remove all instances of \\n where only the n remains\n",
        "    split = [i for i in split if i != \"n\"]\n",
        "    return \" \".join(split)\n",
        "\n",
        "def tokenize(x_train,y_train,x_val,y_val):\n",
        "    word_list = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "    corpus = Counter(word_list)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]                #Only using top 1000 words to train model\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "    # tokenize\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)]  for word in sent.lower().split()\n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
        "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
        "    encoded_train = [1 if label == 2.0 else 0 for label in y_train]            #  Changed from 'positive' to 2.0 for my dataset\n",
        "    encoded_test = [1 if label == 2.0 else 0 for label in y_val]\n",
        "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoizOzvyDh5d",
        "outputId": "6f789b2f-a8fa-42aa-b7c2-ca1c5e3ea5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-13fe6daea044>:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n"
          ]
        }
      ],
      "source": [
        "x_train,y_train,x_test,y_test,vocab = tokenize(x_train,y_train,x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIAkU_HKaH88"
      },
      "source": [
        "# Padding (len 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CGLnBPPmEGn6"
      },
      "outputs": [],
      "source": [
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "x_train_pad = padding_(x_train,500)\n",
        "x_test_pad = padding_(x_test,500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bldHeeBQg2p"
      },
      "source": [
        "# Shuffle Train/Val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1fWmPaxLEOL7"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train)) # len 750 - each element is a list of 500 ints (after padding)\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))   # len 250\n",
        "batch_size = 50    #  number of posts in x_train/x_test must be divisible by batch_size - otherwise, there'll be an error when training model\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)           # shuffled train_data/valid_data\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQBZ_0sMYmu0"
      },
      "source": [
        "# Create Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22RjvTe-ExYO",
        "outputId": "a9c588bd-43c4-4b05-c17d-9f301f35c9aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentRNN(\n",
              "  (embedding): Embedding(1001, 64)\n",
              "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (sig): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim, num_layers=no_layers, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(embeds.shape)  #[50, 500, 1000]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "#moving to gpu\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rqSplN5Yszz"
      },
      "source": [
        "# Define learning rate and accuracy func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "51CKkiKLFfNi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "lr = 0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "    # return accuracy_score(label, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RntBfJYw0N"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lX9mV3XFvjE",
        "outputId": "9b93d624-b8ff-4bb4-9f22-1eccadd67ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 0.9250793650793651\n",
            "Train loss: 0.2549446557485868\n",
            "Val loss: 0.9485714285714286\n",
            "Val acc: 0.20457159940685546\n",
            "Epoch 1: 4.036 seconds\n",
            "Train acc: 0.9476190476190476\n",
            "Train loss: 0.20860992037942486\n",
            "Val loss: 0.9485714285714286\n",
            "Val acc: 0.20782707631587982\n",
            "Epoch 2: 3.983 seconds\n",
            "Train acc: 0.9476190476190476\n",
            "Train loss: 0.19824900705781248\n",
            "Val loss: 0.9485714285714286\n",
            "Val acc: 0.2133760846086911\n",
            "Epoch 3: 3.911 seconds\n",
            "Train acc: 0.9476190476190476\n",
            "Train loss: 0.20558967938025793\n",
            "Val loss: 0.9485714285714286\n",
            "Val acc: 0.19838525567735946\n",
            "Epoch 4: 3.919 seconds\n",
            "Train acc: 0.9485714285714286\n",
            "Train loss: 0.18521683621737692\n",
            "Val loss: 0.9457142857142857\n",
            "Val acc: 0.16567846706935338\n",
            "Epoch 5: 3.943 seconds\n",
            "Train acc: 0.9552380952380952\n",
            "Train loss: 0.14712141690746186\n",
            "Val loss: 0.9514285714285714\n",
            "Val acc: 0.16439263735498702\n",
            "Epoch 6: 3.914 seconds\n",
            "Train acc: 0.9615873015873015\n",
            "Train loss: 0.12000548325124241\n",
            "Val loss: 0.9514285714285714\n",
            "Val acc: 0.18229569920471736\n",
            "Epoch 7: 3.934 seconds\n",
            "Train acc: 0.9666666666666667\n",
            "Train loss: 0.09680424033412857\n",
            "Val loss: 0.9342857142857143\n",
            "Val acc: 0.20060142342533385\n",
            "Epoch 8: 3.872 seconds\n",
            "Train acc: 0.9809523809523809\n",
            "Train loss: 0.06680397509730288\n",
            "Val loss: 0.94\n",
            "Val acc: 0.20060809648462705\n",
            "Epoch 9: 3.891 seconds\n",
            "Train acc: 0.9812698412698413\n",
            "Train loss: 0.05318871707773753\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.24003759026527405\n",
            "Epoch 10: 3.929 seconds\n",
            "Train acc: 0.9873015873015873\n",
            "Train loss: 0.0346995101750104\n",
            "Val loss: 0.9371428571428572\n",
            "Val acc: 0.26311434007116724\n",
            "Epoch 11: 3.895 seconds\n",
            "Train acc: 0.9949206349206349\n",
            "Train loss: 0.013467851691530086\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.33696318151695387\n",
            "Epoch 12: 3.889 seconds\n",
            "Train acc: 0.993968253968254\n",
            "Train loss: 0.016206926622775397\n",
            "Val loss: 0.9371428571428572\n",
            "Val acc: 0.3444874250063939\n",
            "Epoch 13: 3.926 seconds\n",
            "Train acc: 0.9974603174603175\n",
            "Train loss: 0.01094129305870627\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.3783774418490274\n",
            "Epoch 14: 3.850 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.0016430807254292877\n",
            "Val loss: 0.9228571428571428\n",
            "Val acc: 0.4662734663912228\n",
            "Epoch 15: 3.892 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.0004166392730743003\n",
            "Val loss: 0.9228571428571428\n",
            "Val acc: 0.5140175138201032\n",
            "Epoch 16: 3.934 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.0002445567859843048\n",
            "Val loss: 0.9228571428571428\n",
            "Val acc: 0.5235110047672477\n",
            "Epoch 17: 3.935 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.00019027246136309777\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.53062976104515\n",
            "Epoch 18: 3.937 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.00012561285893752938\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5351965469973428\n",
            "Epoch 19: 3.930 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 0.0001231713536937951\n",
            "Val loss: 0.9228571428571428\n",
            "Val acc: 0.557943365403584\n",
            "Epoch 20: 3.942 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 8.830291595326603e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5602802336215973\n",
            "Epoch 21: 3.937 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 6.856729297642088e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5694835228579385\n",
            "Epoch 22: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 6.585746456357088e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5731478631496429\n",
            "Epoch 23: 3.942 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 5.97288091364409e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5736743658781052\n",
            "Epoch 24: 3.913 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.853163835875984e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5932498276233673\n",
            "Epoch 25: 3.880 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.2244446185288814e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.5965349674224854\n",
            "Epoch 26: 3.904 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.788220836155695e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.5893072251762662\n",
            "Epoch 27: 3.912 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.690566539998621e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6048744257007327\n",
            "Epoch 28: 3.919 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.8829350975727375e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6022579100515161\n",
            "Epoch 29: 3.917 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.9826158648603947e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.617720373604113\n",
            "Epoch 30: 3.929 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.7014919894662897e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6216187854303955\n",
            "Epoch 31: 3.937 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.2438134342409617e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.623559538615934\n",
            "Epoch 32: 3.940 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.1206656020651627e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6300382486411503\n",
            "Epoch 33: 3.942 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.0372181682302782e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6336774059704372\n",
            "Epoch 34: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.792376309143918e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6393665671348572\n",
            "Epoch 35: 3.925 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.7168796455099787e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6275243397269931\n",
            "Epoch 36: 3.933 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.659907961910658e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.632542046053069\n",
            "Epoch 37: 3.937 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.4620062072950416e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6372162827423641\n",
            "Epoch 38: 3.917 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.3097668713834581e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6426469811371395\n",
            "Epoch 39: 3.930 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.1547760084881919e-05\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.6406469706978116\n",
            "Epoch 40: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.2741013502847405e-05\n",
            "Val loss: 0.9257142857142857\n",
            "Val acc: 0.6585191573415484\n",
            "Epoch 41: 3.931 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.0874462048608592e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6579050293990544\n",
            "Epoch 42: 3.921 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.1077909710091637e-05\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6608913966587612\n",
            "Epoch 43: 3.916 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 8.790590760283868e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6635355736528125\n",
            "Epoch 44: 3.920 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 9.353021374365545e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6645543447562626\n",
            "Epoch 45: 3.948 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 9.51312590731259e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.6601933352649212\n",
            "Epoch 46: 3.929 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 7.481282779504938e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6772282080990928\n",
            "Epoch 47: 3.919 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 7.671019304888476e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6706813062940326\n",
            "Epoch 48: 3.925 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 7.5292470217478005e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6778015366804279\n",
            "Epoch 49: 3.940 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 6.906472984314499e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6753920614719391\n",
            "Epoch 50: 3.915 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 7.317329932664593e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6798843666911125\n",
            "Epoch 51: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 6.411256392658245e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6865270222936358\n",
            "Epoch 52: 3.934 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 5.83003426823231e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.6810090595390648\n",
            "Epoch 53: 3.944 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 5.12066418603729e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.6829546179090228\n",
            "Epoch 54: 3.922 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.987535337224674e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6965070941618511\n",
            "Epoch 55: 3.921 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 5.302045783609505e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.6833998944078173\n",
            "Epoch 56: 3.935 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 5.012786966780989e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7020714921610696\n",
            "Epoch 57: 3.929 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.6627928913804925e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6954171274389539\n",
            "Epoch 58: 3.928 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.520315115545571e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6997944584914616\n",
            "Epoch 59: 3.936 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.650694287582675e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.70735061125075\n",
            "Epoch 60: 3.909 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 4.900117371815011e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.6987379491329193\n",
            "Epoch 61: 3.925 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.506094493505294e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.70069431711134\n",
            "Epoch 62: 3.935 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.665194660986763e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7034844032355717\n",
            "Epoch 63: 3.948 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.337300107713894e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7174239946263177\n",
            "Epoch 64: 3.923 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.155429348160916e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7176728268220488\n",
            "Epoch 65: 3.938 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.1029449136411363e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7103828266263008\n",
            "Epoch 66: 3.919 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.9874339345172174e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7084152379206249\n",
            "Epoch 67: 3.919 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.360382267586944e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7187170663050243\n",
            "Epoch 68: 3.941 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.9042299734065016e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7295893899032048\n",
            "Epoch 69: 3.945 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.2762013413430972e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7293365044253213\n",
            "Epoch 70: 3.942 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.5655804589650726e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7317234831196922\n",
            "Epoch 71: 3.913 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 3.1962966742203063e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7272818854876927\n",
            "Epoch 72: 3.913 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.3564700588019422e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7360005883425141\n",
            "Epoch 73: 3.936 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.3885908101798727e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7337470948696136\n",
            "Epoch 74: 3.943 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.995742938077687e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7276326609509332\n",
            "Epoch 75: 3.946 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.3727834622554333e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7328636859144483\n",
            "Epoch 76: 3.917 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 2.2351854940754693e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7432620099612645\n",
            "Epoch 77: 3.926 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.9915079585571797e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7408402817589896\n",
            "Epoch 78: 3.914 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.8201765287235575e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.734298397387777\n",
            "Epoch 79: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.648003082740009e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7397423876183373\n",
            "Epoch 80: 3.934 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.6187965091570992e-06\n",
            "Val loss: 0.9342857142857143\n",
            "Val acc: 0.7350883207150868\n",
            "Epoch 81: 3.941 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.7069013014161245e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7447267270513943\n",
            "Epoch 82: 3.939 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.6691794754146868e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7450168473379952\n",
            "Epoch 83: 3.940 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.530861327509344e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7482720868928092\n",
            "Epoch 84: 3.940 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.4281982010014337e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7559738265616553\n",
            "Epoch 85: 3.942 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.3675865064879107e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7566012016364506\n",
            "Epoch 86: 3.925 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.5372614572899895e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7508571105343955\n",
            "Epoch 87: 3.936 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.2589324574001073e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7648352985935551\n",
            "Epoch 88: 3.923 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.404260194884421e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7606176223073687\n",
            "Epoch 89: 3.926 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.4417581040572173e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.753926864692143\n",
            "Epoch 90: 3.927 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.0520197174213217e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7672233219657626\n",
            "Epoch 91: 3.940 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.1368104043727033e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7610865065029689\n",
            "Epoch 92: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.1956018945673868e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7616193762847355\n",
            "Epoch 93: 3.939 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.0438884350789868e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7712212715830121\n",
            "Epoch 94: 3.947 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.0801319029320034e-06\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.769387401521921\n",
            "Epoch 95: 3.948 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 9.925527801139349e-07\n",
            "Val loss: 0.9285714285714286\n",
            "Val acc: 0.7806423412902015\n",
            "Epoch 96: 3.932 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 1.0619363326751473e-06\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7683019406561341\n",
            "Epoch 97: 3.930 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 8.088155350974159e-07\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7707049591200692\n",
            "Epoch 98: 3.933 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 8.815276708948283e-07\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7785194935942334\n",
            "Epoch 99: 3.935 seconds\n",
            "Train acc: 1.0\n",
            "Train loss: 8.43027654428942e-07\n",
            "Val loss: 0.9314285714285714\n",
            "Val acc: 0.7834241219929287\n",
            "Epoch 100: 3.944 seconds\n"
          ]
        }
      ],
      "source": [
        "clip = 5\n",
        "epochs = 100\n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "for epoch in range(epochs):\n",
        "    a = time.time()\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            val_loss = criterion(output.squeeze(), labels.float())\n",
        "            val_losses.append(val_loss.item())\n",
        "            accuracy = acc(output,labels)\n",
        "            #add f1 for test predictions\n",
        "            val_acc += accuracy\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    b = time.time()\n",
        "    t = b-a\n",
        "    print(\"Train acc:\", epoch_train_acc)\n",
        "    print(\"Train loss:\", epoch_train_loss)\n",
        "    print(\"Val loss:\", epoch_val_acc)\n",
        "    print(\"Val acc:\", epoch_val_loss)\n",
        "    print(f'Epoch {epoch+1}: {t:.3f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E59uS4HIFyHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604fc6b9-342f-4abe-848e-435429c703f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’ve been meaning to make a post on here for a while, but I’m happy to finally say I’ve been vape free for 61 days now! I’d only been previously vaping for about a year so my case isn’t as bad as some, but I hated how attached I was to my vape and wanted to quit. I knew it wasn’t good for my health and since I’m into some singing and vocal performance, I knew I needed to stop soon. \n",
            "\n",
            "Family and friends were encouraging me for a few months but it didn’t help out too much. Sometimes I’d try to stop for a day or two, but I’d see my vape and get right back to it. But, one day I thought “I’m not gonna let this control me.” Instead of “I think I’m going to quit”, I said “I WILL quit vaping.” \n",
            "\n",
            "The first couple days it was terrible. By day 3 I got so close to going to buy one, but I thought back to myself and kept going. I will say though, once I made it to 1 week, it got a lot easier from there. I still get headaches and urges even now, but they normally go away after a few minutes. \n",
            "\n",
            "For anyone who sees this trying to quit, YOU CAN DO IT! I BELIEVE IN YOU! I never thought I would quit this soon, but if you really commit and do it for yourself, you got this. \n",
            "\n",
            "Some things that helped me were day 1, I took ALL my vapes to my school and threw everything out in the trash there. Not having it in my trash at home took out the option of reaching back for it later. Having them out of sight helped out tremendously. \n",
            "\n",
            "Also the truth initiative text line helped out a lot too. If you’re like me and don’t feel comfortable bringing your addiction up to anyone, having encouraging texts come in felt like someone was pushing me to keep going. I still use it even now if I ever get urges. If u want it, text DITCHVAPE to 88709\n",
            "\n",
            "Stay strong everyone! I love you all👊🏾\n",
            "======================================================================\n",
            "Actual sentiment is  : 4270   NaN\n",
            "4271   NaN\n",
            "Name: labels, dtype: float64\n",
            "======================================================================\n",
            "[1.4990237104939297e-06, 0.0463259331882, 3.0458867428251324e-08, 1.7292758229814353e-06]\n"
          ]
        }
      ],
      "source": [
        "def predict_text(texts):\n",
        "  l = []\n",
        "  for text in texts:\n",
        "    word_seq = np.array([vocab[preprocess_string(word)] for word in text.split()\n",
        "                      if preprocess_string(word) in vocab.keys()])\n",
        "    word_seq = np.expand_dims(word_seq,axis=0)\n",
        "    pad =  torch.from_numpy(padding_(word_seq,500))\n",
        "    inputs = pad.to(device)\n",
        "    batch_size = 1\n",
        "    h = model.init_hidden(batch_size)\n",
        "    h = tuple([each.data for each in h])\n",
        "    output, h = model(inputs, h)\n",
        "    l.append(output.item())\n",
        "  return l\n",
        "\n",
        "index = 4274\n",
        "print(df['selftext'][4271])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df[\"labels\"][4270:4272]}')\n",
        "print('='*70)\n",
        "pro = predict_text(df['selftext'][4270:index])\n",
        "print(pro)\n",
        "# status = \"positive\" if pro > 0.5 else \"negative\"\n",
        "# pro = (1 - pro) if status == \"negative\" else pro\n",
        "# print(f'Predicted sentiment is {status} with a probability of {pro}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}