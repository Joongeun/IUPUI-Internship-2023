{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5945,
          "status": "ok",
          "timestamp": 1692039335415,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          },
          "user_tz": 240
        },
        "id": "SEAmDvjK7kqr",
        "outputId": "91cf9982-2bed-4ad5-96f8-3cd9017e75e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tbB3k8T075h8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692039446997,
          "user_tz": 240,
          "elapsed": 162,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_text(sentences, stopwords):\n",
        "  sentences = sentences.lower()\n",
        "  split = re.findall(r\"[\\w']+|[!?]\", sentences)\n",
        "  i = 0\n",
        "  while i < len(split)-1:\n",
        "    #Check if there are multiple exclammation/question marks and only leave 1 of them in the sentence\n",
        "    if split[i] == split[i+1]:\n",
        "      del split[i+1]\n",
        "      i-=1\n",
        "    i+= 1\n",
        "  # Remove all instances of \\n where only the n remains\n",
        "  split = [i for i in split if i != \"n\"]\n",
        "  return \" \".join(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GvQbSYXWAoai",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692039448609,
          "user_tz": 240,
          "elapsed": 2,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(length, ngram, d):\n",
        "  X = df['selftext'][0:length].tolist()\n",
        "  Y = df['labels'][0:length].tolist()\n",
        "  for i, post in enumerate(X):\n",
        "    X[i] = clean_text(post)\n",
        "  if ngram == \"binary\":\n",
        "    vectorizer = CountVectorizer(vocabulary = d, binary=True)\n",
        "  elif ngram == \"unigram\":\n",
        "    vectorizer = CountVectorizer(vocabulary = d)\n",
        "  else:\n",
        "    vectorizer = CountVectorizer(vocabulary = d, ngram_range = (2, 2), binary=False) #Bigram/binary\n",
        "  bag = vectorizer.fit_transform(X)\n",
        "  #len(bag) == 100 - vectors of posts\n",
        "  bag = bag.toarray()\n",
        "  skf = StratifiedKFold(n_splits=10)\n",
        "  skf.get_n_splits(bag, Y)\n",
        "  X_train, y_train, X_test, y_test = [], [], [], []\n",
        "  for i, (train_index, test_index) in enumerate(skf.split(bag, Y)):\n",
        "    xtrain, ytrain, xtest, ytest = [], [], [], []\n",
        "    for j in train_index:\n",
        "      xtrain.append(bag[j])\n",
        "      ytrain.append(Y[j])\n",
        "    for j in test_index:\n",
        "      xtest.append(bag[j])\n",
        "      ytest.append(Y[j])\n",
        "    X_train.append(xtrain)\n",
        "    y_train.append(ytrain)\n",
        "    X_test.append(xtest)\n",
        "    y_test.append(ytest)\n",
        "  return X_train, y_train, X_test, y_test\n",
        "  #for 100 posts:\n",
        "  #len(X_train, y_train) == 10, each of the 10 folds have 90 np vector arrays (posts)\n",
        "  #len(X_test, y_test) == 10, 10 np arrays per fold - 100 posts total\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "# print(len(X_test))\n",
        "# print(X_test[0])\n",
        "# print(len(X_test[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rlOGiip776uL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692034409519,
          "user_tz": 240,
          "elapsed": 156,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        }
      },
      "outputs": [],
      "source": [
        "def classify(X_train, y_train, X_test, y_test, length, count, label):\n",
        "  w = {0:count, 1:length-count}\n",
        "  lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='ovr', max_iter = 1000000, class_weight = w)\n",
        "\n",
        "  # Use metrics.accuracy_score to measure the score\n",
        "  #'macro' favors minority, 'weighted' favors majority, 'micro' favors none (when you have multiclass)\n",
        "  scores = []\n",
        "  precision = []\n",
        "  recall = []\n",
        "  for i in range(0, 10):\n",
        "    #Get 9 folds for training\n",
        "    xtrain = X_train[i]\n",
        "    ytrain = y_train[i]\n",
        "    lr.fit(xtrain, ytrain)\n",
        "\n",
        "    xtest = X_test[i]\n",
        "    ytest = y_test[i]\n",
        "    y_predict = lr.predict(xtest)\n",
        "    if label in [1.0, 4.0]:\n",
        "      score = f1_score(ytest, y_predict, labels=np.unique(y_predict))\n",
        "    else:\n",
        "      score = f1_score(ytest, y_predict, labels=np.unique(y_predict))\n",
        "    scores.append(\"%.3f\" %score)\n",
        "    prec = precision_score(ytest, y_predict)\n",
        "    precision.append(\"%.3f\"%prec)\n",
        "    rec = recall_score(ytest, y_predict)\n",
        "    recall.append(rec)\n",
        "  return lr, scores, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making dictionary to put as the argument for the \"vocabulary\" parameter in CountVectorizer - need this to get words that the model has not yet seen\n",
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "d=[]\n",
        "count = 0\n",
        "for i in df['selftext']:\n",
        "  for word in i.split():\n",
        "    if word not in d:\n",
        "      d.append(word)\n",
        "print(len(d)) #41716\n",
        "\n",
        "def average(lst):\n",
        "  s = 0\n",
        "  for i in lst:\n",
        "    s += float(i)\n",
        "  return s/len(lst)"
      ],
      "metadata": {
        "id": "PFF8eJY0DFM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692039393354,
          "user_tz": 240,
          "elapsed": 51223,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "c083cb55-005d-4b41-c0c2-823bb4b6cc8c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veVPk0C17pnc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692040278999,
          "user_tz": 240,
          "elapsed": 824293,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "bc856366-32a7-4a63-a9dc-f99370597ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 posts: \n",
            "---------------\n",
            "Scores for 2.0 : \n",
            "\n",
            "Precision 0.47850000000000004 0.035699999999999996 0.5088\n",
            "Recall 0.36052631578947375 0.7 0.3283625730994152\n",
            "Unigram F1: 0.405\n",
            "['0.556', '0.357', '0.438', '0.471', '0.370', '0.343', '0.286', '0.400', '0.486', '0.343']\n",
            "---------------\n",
            "Bigram F1: 0.069\n",
            "['0.098', '0.098', '0.098', '0.098', '0.098', '0.098', '0.098', '0.000', '0.000', '0.000']\n",
            "---------------\n",
            "Binary F1: 0.390\n",
            "['0.432', '0.333', '0.452', '0.471', '0.345', '0.414', '0.357', '0.296', '0.375', '0.424']\n",
            "---------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "for i in [2.0]:\n",
        "  count = 0\n",
        "  for ind in df.index:\n",
        "    if df.loc[ind, \"labels\"] != i:\n",
        "      df.loc[ind, \"labels\"] = 0\n",
        "    else:\n",
        "      df.loc[ind, \"labels\"] = 1\n",
        "      count += 1\n",
        "  length = 3500\n",
        "  unigram, bigram, binary = [], [], []\n",
        "  while length < 3600:\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"unigram\", d)\n",
        "    lr1, unigram, p1, r1 = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"bigram\", d)\n",
        "    lr2, bigram, p2, r2 = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"binary\", d)\n",
        "    lr3, binary, p3, r3 = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    print(length, \"posts: \")\n",
        "    print(\"-\"*15)\n",
        "    print(\"Scores for\", i, \": \\n\")\n",
        "    print(\"Precision\", average(p1), average(p2), average(p3))\n",
        "    print(\"Recall\", average(r1), average(r2), average(r3))\n",
        "    print(\"Unigram F1: %.3f\"%average(unigram))\n",
        "    print(unigram)\n",
        "    print(\"-\"*15)\n",
        "    print(\"Bigram F1: %.3f\"%average(bigram))\n",
        "    print(bigram)\n",
        "    print(\"-\"*15)\n",
        "    print(\"Binary F1: %.3f\"%average(binary))\n",
        "    print(binary)\n",
        "    print(\"-\"*15)\n",
        "    print(\"\\n\")\n",
        "    length += 500\n",
        "  df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HfZZkU8vjX9X",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692040279959,
          "user_tz": 240,
          "elapsed": 976,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        }
      },
      "outputs": [],
      "source": [
        "def a(ngram):\n",
        "  vectorizer = 0\n",
        "  if ngram == \"binary\":\n",
        "    vectorizer = CountVectorizer(vocabulary=d, binary=True)\n",
        "  elif ngram == \"unigram\":\n",
        "    vectorizer = CountVectorizer(vocabulary=d)\n",
        "  else: #bigram\n",
        "    vectorizer = CountVectorizer(vocabulary=d, ngram_range = (2, 2))\n",
        "  return vectorizer\n",
        "\n",
        "def b(vectorizer):\n",
        "  X = np.array(df['selftext'][3100:7751].tolist())\n",
        "  for i, post in enumerate(X):\n",
        "    X[i] = clean_text(post)\n",
        "\n",
        "  bag = vectorizer.fit_transform(X)\n",
        "  bag = bag.toarray()\n",
        "  return bag\n",
        "\n",
        "count = 0\n",
        "i = 2.0\n",
        "for ind in df.index:\n",
        "  if df.loc[ind, \"labels\"] != i:\n",
        "      df.loc[ind, \"labels\"] = 0\n",
        "  else:\n",
        "    df.loc[ind, \"labels\"] = 1\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting number of advice posts (1.0 for advice, 0.0 for anything else)\n",
        "vectorizer = a(\"unigram\")\n",
        "data = b(vectorizer)\n",
        "res1 = lr1.predict(data)\n",
        "print(np.count_nonzero(res1 == 1.0))\n",
        "\n",
        "vectorizer = a(\"bigram\")\n",
        "data = b(vectorizer)\n",
        "res2 = lr2.predict(data)\n",
        "print(np.count_nonzero(res2 == 1.0))\n",
        "\n",
        "vectorizer = a(\"binary\")\n",
        "data = b(vectorizer)\n",
        "res3 = lr3.predict(data)\n",
        "print(np.count_nonzero(res3 == 1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q__IVuT3Lh8e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1692040291788,
          "user_tz": 240,
          "elapsed": 11633,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "421de4a5-ff1a-4cf5-8ef9-36602efe8292"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "\n",
        "def p(x):\n",
        "  l = []\n",
        "  for ind, pred in enumerate(x):\n",
        "    if pred == 1.0:\n",
        "      l.append(ind+3100)\n",
        "  return l\n",
        "l1 = p(res1)\n",
        "l3 = p(res3)\n",
        "prediction1 = []\n",
        "prediction2 = []\n",
        "for i in l1:\n",
        "  prediction1.append(df['selftext'][i])\n",
        "for i in l3:\n",
        "  prediction2.append(df['selftext'][i])\n",
        "for i in range(11):\n",
        "  prediction2.append(\"\")\n",
        "\n",
        "c = []\n",
        "for i in l1:\n",
        "  if l3.count(i) > 0:\n",
        "    c.append(df['selftext'][i])\n",
        "for i in range(57):\n",
        "  c.append(\"\")\n",
        "c.append(l1)\n",
        "c.append(l3)\n",
        "print(len(prediction1), len(prediction2), len(c))\n",
        "d = {'unigram': prediction1, 'binary': prediction2, 'both': c}\n",
        "\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "# saving the dataframe\n",
        "df.to_csv('predictions.csv')"
      ],
      "metadata": {
        "id": "TpDJvuuyC314"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df['selftext'][501:3500].tolist())\n",
        "bag = vectorizer.fit_transform(X)\n",
        "bag = bag.toarray()\n",
        "y_predict = lr2.predict(bag)\n",
        "ytest = np.array(df['labels'][501:3500].tolist())\n",
        "score = f1_score(ytest, y_predict, labels=np.unique(y_predict))\n",
        "print(score)"
      ],
      "metadata": {
        "id": "NvIrxv9wLHjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df['selftext'][1:100].tolist())\n",
        "vectorizer = CountVectorizer()\n",
        "bag = vectorizer.fit_transform(X)\n",
        "print(bag.toarray()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp7FW1NPkE3C",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1691966995215,
          "user_tz": 240,
          "elapsed": 236,
          "user": {
            "displayName": "Joongeun Choi",
            "userId": "16179959334871840414"
          }
        },
        "outputId": "12fffbc5-ac60-4cd8-c58d-c464c51cf064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1vxc1Uin9QBhKGJwpwKhcYs8zU9KH9d-V",
          "timestamp": 1690355592715
        },
        {
          "file_id": "1zEI3cEllICPUaXCHHVY1MymBxXvp12fS",
          "timestamp": 1689834896579
        }
      ],
      "authorship_tag": "ABX9TyMi8YUQfc+TmdfMcME9T0y4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
