{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joongeun/Internship/blob/main/Fixed_Bag_Of_Words_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAmDvjK7kqr",
        "outputId": "5082017a-8b5a-48ed-f8b3-92336b417225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbB3k8T075h8"
      },
      "outputs": [],
      "source": [
        "def clean_text(sentences):\n",
        "  cleaned_sentence = \"\"\n",
        "  sentences = sentences.lower().split()\n",
        "  ##removing stop words\n",
        "  words = [i for i in sentences if i not in stopwords.words('english')]\n",
        "  words = \" \".join(words)               ##joining our words back to sentences\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvQbSYXWAoai"
      },
      "outputs": [],
      "source": [
        "def preprocess(length, ngram):\n",
        "  X = df['selftext'][0:length].tolist()\n",
        "  Y = df['labels'][0:length].tolist()\n",
        "  for i, post in enumerate(X):\n",
        "    X[i] = clean_text(post)\n",
        "  if ngram == \"binary\":\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "  elif ngram == \"unigram\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else:\n",
        "    vectorizer = CountVectorizer(ngram_range = (2, 2), binary=True) #Bigram/binary\n",
        "  bag = vectorizer.fit_transform(X)\n",
        "  #len(bag) == 100 - vectors of posts\n",
        "  bag = bag.toarray()\n",
        "  skf = StratifiedKFold(n_splits=10)\n",
        "  skf.get_n_splits(bag, Y)\n",
        "  X_train, y_train, X_test, y_test = [], [], [], []\n",
        "  for i, (train_index, test_index) in enumerate(skf.split(bag, Y)):\n",
        "    xtrain, ytrain, xtest, ytest = [], [], [], []\n",
        "    for i in train_index:\n",
        "      xtrain.append(bag[i])\n",
        "      ytrain.append(Y[i])\n",
        "    for i in test_index:\n",
        "      xtest.append(bag[i])\n",
        "      ytest.append(Y[i])\n",
        "    X_train.append(xtrain)\n",
        "    y_train.append(ytrain)\n",
        "    X_test.append(xtest)\n",
        "    y_test.append(ytest)\n",
        "  return X_train, y_train, X_test, y_test\n",
        "  #len(X_train, y_train) == 10, each of the 10 sublists have 90 np vector arrays (posts)\n",
        "  #len(X_test, y_test) == 10, 10 np arrays per fold - 100 posts total\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "# print(len(X_test))\n",
        "# print(X_test[0])\n",
        "# print(len(X_test[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlOGiip776uL"
      },
      "outputs": [],
      "source": [
        "def classify(X_train, y_train, X_test, y_test, length, count, label):\n",
        "  w = {0:length-count, 1:count}\n",
        "  lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='ovr', max_iter = 1000000, class_weight = w)\n",
        "\n",
        "  # Use metrics.accuracy_score to measure the score\n",
        "  #'macro' favors minority, 'weighted' favors majority, 'micro' favors none (when you have multiclass)\n",
        "  scores = []\n",
        "  for i in range(0, 10):\n",
        "    #Get 9 folds for training\n",
        "    xtrain = X_train[i]\n",
        "    ytrain = y_train[i]\n",
        "    lr.fit(xtrain, ytrain)\n",
        "\n",
        "    xtest = X_test[i]\n",
        "    ytest = y_test[i]\n",
        "    y_predict = lr.predict(xtest)\n",
        "    if label in [1.0, 4.0]:\n",
        "      score = f1_score(ytest, y_predict, average='micro', labels=np.unique(y_predict))\n",
        "    else:\n",
        "      score = f1_score(ytest, y_predict, average='macro', labels=np.unique(y_predict))\n",
        "    scores.append(\"%.3f\" %score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "veVPk0C17pnc",
        "outputId": "d61ab204-8db2-4de7-ce47-efd4d71bb625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores for 2.0 : \n",
            "\n",
            "0.801\n",
            "0.586\n",
            "0.751\n",
            "0.770\n",
            "0.700\n",
            "0.661\n",
            "0.719\n",
            "0.670\n",
            "0.706\n",
            "0.783\n",
            "Average: 7.147000000000001\n",
            "-----------------------\n",
            "0.973\n",
            "0.973\n",
            "0.973\n",
            "0.546\n",
            "0.546\n",
            "0.546\n",
            "0.973\n",
            "0.973\n",
            "0.546\n",
            "0.973\n",
            "Average: 0.8022\n",
            "-----------------------\n",
            "0.740\n",
            "0.630\n",
            "0.764\n",
            "0.748\n",
            "0.678\n",
            "0.630\n",
            "0.679\n",
            "0.670\n",
            "0.653\n",
            "0.706\n",
            "Average: 0.6898\n",
            "\n",
            "\n",
            "Scores for 3.0 : \n",
            "\n",
            "0.488\n",
            "0.606\n",
            "0.534\n",
            "0.484\n",
            "0.630\n",
            "0.656\n",
            "0.534\n",
            "0.520\n",
            "0.638\n",
            "0.534\n",
            "Average: 5.624\n",
            "-----------------------\n",
            "0.978\n",
            "0.978\n",
            "0.978\n",
            "0.978\n",
            "0.978\n",
            "0.978\n",
            "0.976\n",
            "0.976\n",
            "0.976\n",
            "0.976\n",
            "Average: 0.9771999999999998\n",
            "-----------------------\n",
            "0.556\n",
            "0.606\n",
            "0.483\n",
            "0.486\n",
            "0.630\n",
            "0.561\n",
            "0.586\n",
            "0.581\n",
            "0.647\n",
            "0.484\n",
            "Average: 0.562\n",
            "\n",
            "\n",
            "Scores for 4.0 : \n",
            "\n",
            "0.727\n",
            "0.740\n",
            "0.687\n",
            "0.740\n",
            "0.700\n",
            "0.737\n",
            "0.673\n",
            "0.693\n",
            "0.717\n",
            "0.710\n",
            "Average: 7.124\n",
            "-----------------------\n",
            "0.693\n",
            "0.703\n",
            "0.657\n",
            "0.673\n",
            "0.677\n",
            "0.687\n",
            "0.693\n",
            "0.670\n",
            "0.670\n",
            "0.677\n",
            "Average: 0.6799999999999999\n",
            "-----------------------\n",
            "0.697\n",
            "0.747\n",
            "0.707\n",
            "0.737\n",
            "0.730\n",
            "0.740\n",
            "0.673\n",
            "0.683\n",
            "0.733\n",
            "0.730\n",
            "Average: 0.7177\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "# for i in list(set(df.labels))[:6]:\n",
        "for i in [2.0, 3.0, 4.0]:\n",
        "  count = 0\n",
        "  for ind in df.index:\n",
        "    # float 0.0, 1.0, 2.0, 3.0, 4.0, 5.0 - represents: general, question, advice, encouragement, experience, bragging posts\n",
        "    if df.loc[ind, \"labels\"] != i:\n",
        "      df.loc[ind, \"labels\"] = 0\n",
        "    else:\n",
        "      df.loc[ind, \"labels\"] = 1\n",
        "      count += 1\n",
        "  length = 3000\n",
        "  unigram, bigram, binary = [], [], []\n",
        "  while length < 3100:\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"unigram\")\n",
        "    unigram = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"bigram\")\n",
        "    bigram = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    X_train, y_train, X_test, y_test = preprocess(length, \"binary\")\n",
        "    binary = classify(X_train, y_train, X_test, y_test, length, count, i)\n",
        "    length += 100\n",
        "  print(\"Scores for\", i, \": \\n\")\n",
        "  sum = 0\n",
        "  for i in unigram:\n",
        "      print(i)\n",
        "      sum += float(i)\n",
        "  print(\"Average: \" + str(sum))\n",
        "  print(\"-----------------------\")\n",
        "  sum = 0\n",
        "  for i in bigram:\n",
        "    print(i)\n",
        "    sum += float(i)\n",
        "  print(\"Average: \" + str(sum/10))\n",
        "  print(\"-----------------------\")\n",
        "  sum = 0\n",
        "  for i in binary:\n",
        "    print(i)\n",
        "    sum += float(i)\n",
        "  print(\"Average: \" + str(sum/10))\n",
        "  print(\"\\n\")\n",
        "  df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OtCs_oRCH7a",
        "outputId": "18ed72aa-cce6-4670-e168-c547aa359fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1046 103 90 675 25\n",
            "61\n",
            "{0: 1325, 1: 675}\n"
          ]
        }
      ],
      "source": [
        "#test to see how many of each post there are\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "labels = list(df.labels)\n",
        "questions = labels.count(1.0)\n",
        "advice = labels.count(2.0)\n",
        "encouragement  = labels.count(3.0)\n",
        "experience = labels.count(4.0)\n",
        "bragging = labels.count(5.0)\n",
        "print(questions, advice, encouragement, experience, bragging)\n",
        "print(labels.count(0.0))\n",
        "ratio = {0:2000-experience, 1:experience}\n",
        "print(ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVzrXp604rxk"
      },
      "source": [
        "Test model on rest of CSV data\n",
        "Error: X has 12867 features, but LogisticRegression is expecting 11214 features as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WadjXMTEk78C"
      },
      "outputs": [],
      "source": [
        "def test(length, ngram, count, label):\n",
        "  X = df['selftext'][0:length].tolist()\n",
        "  Y = df['labels'][0:length].tolist()\n",
        "  for i, post in enumerate(X):\n",
        "    X[i] = clean_text(post)\n",
        "  if ngram == \"binary\":\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "  elif ngram == \"unigram\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else:\n",
        "    vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
        "  bag = vectorizer.fit_transform(X)\n",
        "  bag = bag.toarray()\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=10)\n",
        "  skf.get_n_splits(bag, Y)\n",
        "\n",
        "  for i, (train_index, test_index) in enumerate(skf.split(bag, Y)):\n",
        "    # print(i)\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "    for i in train_index:\n",
        "      X_train.append(bag[i])\n",
        "      y_train.append(Y[i])\n",
        "    for i in test_index:\n",
        "      X_test.append(bag[i])\n",
        "      y_test.append(Y[i])\n",
        "    (X_train, y_train, X_test, y_test) = map(np.array, (X_train, y_train, X_test, y_test))\n",
        "  w = {0:2000-count, 1:count}\n",
        "  lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='ovr', max_iter = 100000, class_weight = w)\n",
        "  #\n",
        "  # Fit the model\n",
        "  #\n",
        "  lr.fit(X_train, y_train)\n",
        "  #\n",
        "  # Create the predictions\n",
        "  #\n",
        "  y_predict = lr.predict(X_test)\n",
        "\n",
        "  # Use metrics.accuracy_score to measure the score\n",
        "  #'macro' favors minority, 'weighted' favors majority, 'micro' favors none (when you have multiclass)\n",
        "  if label in [1.0, 4.0]:\n",
        "    score = f1_score(y_test, y_predict, average='micro', labels=np.unique(y_predict))\n",
        "  else:\n",
        "    score = f1_score(y_test, y_predict, average='macro', labels=np.unique(y_predict))\n",
        "  return \"%.3f\" %score, lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfZZkU8vjX9X"
      },
      "outputs": [],
      "source": [
        "def a(ngram):\n",
        "  vectorizer = 0\n",
        "  if ngram == \"binary\":\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "  elif ngram == \"unigram\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else: #bigram\n",
        "    vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
        "  return vectorizer\n",
        "\n",
        "def b(vectorizer):\n",
        "  X = np.array(df['selftext'][3100:7751].tolist())\n",
        "  for i, post in enumerate(X):\n",
        "    X[i] = clean_text(post)\n",
        "\n",
        "  bag = vectorizer.fit_transform(X)\n",
        "  bag = bag.toarray()\n",
        "  return bag\n",
        "\n",
        "df = pd.read_csv('/content/Labeled Posts - preprocessed_csv.csv')\n",
        "count = 0\n",
        "i = 2.0\n",
        "for ind in df.index:\n",
        "  if df.loc[ind, \"labels\"] != i:\n",
        "      df.loc[ind, \"labels\"] = 0\n",
        "  else:\n",
        "    df.loc[ind, \"labels\"] = 1\n",
        "    count += 1\n",
        "\n",
        "vectorizer = a(\"unigram\")\n",
        "unigram, lr1 = test(3000, \"unigram\", count, i)\n",
        "# bigram, lr2  = test(3000, \"bigram\", count, i)\n",
        "# vectorizer = a(\"binary\")\n",
        "# binary, lr3 = test(3000, \"binary\", count, i)\n",
        "\n",
        "bag = b(vectorizer)\n",
        "data = np.array(bag)\n",
        "\n",
        "print(lr1.predict(data))\n",
        "# print(lr2.predict(data))\n",
        "# print(lr3.predict(data))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVrGBb6rV3e/q+5HG6gCwn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}